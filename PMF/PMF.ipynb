{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.96e+02, 2.42e+02, 3.00e+00],\n",
       "       [1.86e+02, 3.02e+02, 3.00e+00],\n",
       "       [2.20e+01, 3.77e+02, 1.00e+00],\n",
       "       ...,\n",
       "       [2.76e+02, 1.09e+03, 1.00e+00],\n",
       "       [1.30e+01, 2.25e+02, 2.00e+00],\n",
       "       [1.20e+01, 2.03e+02, 3.00e+00]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(file_path='u.data'):\n",
    "    prefer = []\n",
    "    for line in open(file_path, 'r'):\n",
    "        (userid, movieid, rating, ts) = line.split('\\t')\n",
    "        uid = int(userid)\n",
    "        mid = int(movieid)\n",
    "        rat = float(rating)\n",
    "        prefer.append([uid, mid, rat])\n",
    "    data = np.array(prefer)\n",
    "    return data\n",
    "\n",
    "ratings = load_data()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(ratings[:,0])), len(np.unique(ratings[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset density:0.062942\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1673</th>\n",
       "      <th>1674</th>\n",
       "      <th>1675</th>\n",
       "      <th>1676</th>\n",
       "      <th>1677</th>\n",
       "      <th>1678</th>\n",
       "      <th>1679</th>\n",
       "      <th>1680</th>\n",
       "      <th>1681</th>\n",
       "      <th>1682</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944 rows Ã— 1683 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  1673  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1     0.0   0.0   3.0   4.0   0.0   3.0   0.0   4.0   0.0   5.0  ...   0.0   \n",
       "2     0.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "939   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0  ...   0.0   \n",
       "940   0.0   0.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0   3.0  ...   0.0   \n",
       "941   0.0   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "942   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "943   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0   0.0   3.0  ...   0.0   \n",
       "\n",
       "     1674  1675  1676  1677  1678  1679  1680  1681  1682  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "939   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "940   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "941   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "942   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "943   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[944 rows x 1683 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing for matrix R\n",
    "import pandas as pd\n",
    "\n",
    "data = ratings\n",
    "train_data, test_data = train_test_split(data,test_size = 0.25, random_state = 42)\n",
    "NUM_USERS = len(np.unique(ratings[:,0]))+1\n",
    "NUM_ITEMS = len(np.unique(ratings[:,1]))+1\n",
    "print('dataset density:{:f}'.format(len(data)*1.0/(NUM_USERS*NUM_ITEMS)))\n",
    "\n",
    "R = np.zeros([NUM_USERS, NUM_ITEMS])\n",
    "\n",
    "for ele in train_data:\n",
    "    R[int(ele[0]), int(ele[1])] = float(ele[2])\n",
    "train_R = R\n",
    "\n",
    "pd.DataFrame(train_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import copy\n",
    "\n",
    "class PMF():\n",
    "\n",
    "    # initialize some paprameters\n",
    "    def __init__(self, R, lambda_alpha=1e-2, lambda_beta=1e-2, latent_size=50, momuntum=0.8,\n",
    "                 lr=0.001, iters=200, seed=None):\n",
    "        self.lambda_alpha = lambda_alpha\n",
    "        self.lambda_beta = lambda_beta\n",
    "        self.momuntum = momuntum\n",
    "        self.R = R\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.iterations = iters\n",
    "        self.lr = lr\n",
    "        self.I = copy.deepcopy(self.R)\n",
    "        self.I[self.I != 0] = 1\n",
    "\n",
    "        self.U = 0.1*self.random_state.rand(np.size(R, 0), latent_size)\n",
    "        self.V = 0.1*self.random_state.rand(np.size(R, 1), latent_size)\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        # the loss function of the model\n",
    "        loss = 0.5*np.sum(self.I*(self.R-np.dot(self.U, self.V.T))**2) + 0.5*self.lambda_alpha*np.sum(np.square(self.U)) + 0.5*self.lambda_beta*np.sum(np.square(self.V))\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, data):\n",
    "        index_data = np.array([[int(ele[0]), int(ele[1])] for ele in data], dtype=int)\n",
    "        u_features = self.U.take(index_data.take(0, axis=1), axis=0)\n",
    "        v_features = self.V.take(index_data.take(1, axis=1), axis=0)\n",
    "        preds_value_array = np.sum(u_features*v_features, 1)\n",
    "        return preds_value_array\n",
    "\n",
    "\n",
    "    def train(self, train_data=None, vali_data=None):\n",
    "        '''\n",
    "        # training process\n",
    "        :param train_data: train data with [[i,j],...] and this indacates that K[i,j]=rating\n",
    "        :param lr: learning rate\n",
    "        :param iterations: number of iterations\n",
    "        :return: learned V, T and loss_list during iterations\n",
    "        '''\n",
    "        train_loss_list = []\n",
    "        vali_rmse_list = []\n",
    "        last_vali_rmse = None\n",
    "\n",
    "        # monemtum\n",
    "        momuntum_u = np.zeros(self.U.shape)\n",
    "        momuntum_v = np.zeros(self.V.shape)\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            # derivate of Vi\n",
    "            grads_u = np.dot(self.I*(self.R-np.dot(self.U, self.V.T)), -self.V) + self.lambda_alpha*self.U\n",
    "\n",
    "            # derivate of Tj\n",
    "            grads_v = np.dot((self.I*(self.R-np.dot(self.U, self.V.T))).T, -self.U) + self.lambda_beta*self.V\n",
    "\n",
    "            # update the parameters\n",
    "            momuntum_u = (self.momuntum * momuntum_u) + self.lr * grads_u\n",
    "            momuntum_v = (self.momuntum * momuntum_v) + self.lr * grads_v\n",
    "            self.U = self.U - momuntum_u\n",
    "            self.V = self.V - momuntum_v\n",
    "\n",
    "            # training evaluation\n",
    "            train_loss = self.loss()\n",
    "            train_loss_list.append(train_loss)\n",
    "\n",
    "            vali_preds = self.predict(vali_data)\n",
    "            vali_rmse = np.sqrt(np.mean(np.square(vali_data[:,2]-vali_preds)))\n",
    "            vali_rmse_list.append(vali_rmse)\n",
    "\n",
    "            print('traning iteration:{: d} ,loss:{: f}, vali_rmse:{: f}'.format(it, train_loss, vali_rmse))\n",
    "\n",
    "            # if last_vali_rmse and (last_vali_rmse - vali_rmse) < 0:\n",
    "            #     print('convergence at iterations:{: d}'.format(it))\n",
    "            #     break\n",
    "            # else:\n",
    "            #     last_vali_rmse = vali_rmse\n",
    "\n",
    "        return self.U, self.V, train_loss_list, vali_rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters are: reg_u=0.100000, reg_v=0.100000, latent_size=30, lr=0.000030, iters=100\n",
      "traning iteration: 0 ,loss: 494507.554215, vali_rmse: 3.633167\n",
      "traning iteration: 1 ,loss: 493375.556161, vali_rmse: 3.629037\n",
      "traning iteration: 2 ,loss: 491695.555733, vali_rmse: 3.622899\n",
      "traning iteration: 3 ,loss: 489448.562465, vali_rmse: 3.614671\n",
      "traning iteration: 4 ,loss: 486597.115413, vali_rmse: 3.604202\n",
      "traning iteration: 5 ,loss: 483087.577663, vali_rmse: 3.591274\n",
      "traning iteration: 6 ,loss: 478851.726713, vali_rmse: 3.575605\n",
      "traning iteration: 7 ,loss: 473808.032012, vali_rmse: 3.556857\n",
      "traning iteration: 8 ,loss: 467862.976918, vali_rmse: 3.534630\n",
      "traning iteration: 9 ,loss: 460912.780642, vali_rmse: 3.508467\n",
      "traning iteration: 10 ,loss: 452845.898131, vali_rmse: 3.477855\n",
      "traning iteration: 11 ,loss: 443546.711994, vali_rmse: 3.442232\n",
      "traning iteration: 12 ,loss: 432900.866144, vali_rmse: 3.400998\n",
      "traning iteration: 13 ,loss: 420802.704200, vali_rmse: 3.353533\n",
      "traning iteration: 14 ,loss: 407165.235024, vali_rmse: 3.299221\n",
      "traning iteration: 15 ,loss: 391932.908435, vali_rmse: 3.237497\n",
      "traning iteration: 16 ,loss: 375097.189112, vali_rmse: 3.167897\n",
      "traning iteration: 17 ,loss: 356714.402985, vali_rmse: 3.090138\n",
      "traning iteration: 18 ,loss: 336924.545777, vali_rmse: 3.004210\n",
      "traning iteration: 19 ,loss: 315968.679947, vali_rmse: 2.910494\n",
      "traning iteration: 20 ,loss: 294201.291708, vali_rmse: 2.809886\n",
      "traning iteration: 21 ,loss: 272092.782234, vali_rmse: 2.703926\n",
      "traning iteration: 22 ,loss: 250216.588448, vali_rmse: 2.594886\n",
      "traning iteration: 23 ,loss: 229215.938200, vali_rmse: 2.485783\n",
      "traning iteration: 24 ,loss: 209747.681079, vali_rmse: 2.380255\n",
      "traning iteration: 25 ,loss: 192405.476194, vali_rmse: 2.282226\n",
      "traning iteration: 26 ,loss: 177631.569165, vali_rmse: 2.195342\n",
      "traning iteration: 27 ,loss: 165633.893034, vali_rmse: 2.122232\n",
      "traning iteration: 28 ,loss: 156330.365017, vali_rmse: 2.063792\n",
      "traning iteration: 29 ,loss: 149341.511221, vali_rmse: 2.018759\n",
      "traning iteration: 30 ,loss: 144043.569671, vali_rmse: 1.983840\n",
      "traning iteration: 31 ,loss: 139677.853766, vali_rmse: 1.954407\n",
      "traning iteration: 32 ,loss: 135493.465868, vali_rmse: 1.925533\n",
      "traning iteration: 33 ,loss: 130887.241521, vali_rmse: 1.893027\n",
      "traning iteration: 34 ,loss: 125503.784802, vali_rmse: 1.854184\n",
      "traning iteration: 35 ,loss: 119271.113501, vali_rmse: 1.808114\n",
      "traning iteration: 36 ,loss: 112368.621636, vali_rmse: 1.755665\n",
      "traning iteration: 37 ,loss: 105144.642412, vali_rmse: 1.699033\n",
      "traning iteration: 38 ,loss: 98012.761540, vali_rmse: 1.641201\n",
      "traning iteration: 39 ,loss: 91356.145527, vali_rmse: 1.585303\n",
      "traning iteration: 40 ,loss: 85460.079408, vali_rmse: 1.534067\n",
      "traning iteration: 41 ,loss: 80480.276745, vali_rmse: 1.489387\n",
      "traning iteration: 42 ,loss: 76443.615491, vali_rmse: 1.452126\n",
      "traning iteration: 43 ,loss: 73271.597352, vali_rmse: 1.422135\n",
      "traning iteration: 44 ,loss: 70815.202876, vali_rmse: 1.398458\n",
      "traning iteration: 45 ,loss: 68891.477263, vali_rmse: 1.379648\n",
      "traning iteration: 46 ,loss: 67315.308262, vali_rmse: 1.364091\n",
      "traning iteration: 47 ,loss: 65923.039075, vali_rmse: 1.350280\n",
      "traning iteration: 48 ,loss: 64587.048545, vali_rmse: 1.336986\n",
      "traning iteration: 49 ,loss: 63222.030232, vali_rmse: 1.323358\n",
      "traning iteration: 50 ,loss: 61784.528992, vali_rmse: 1.308929\n",
      "traning iteration: 51 ,loss: 60267.579930, vali_rmse: 1.293586\n",
      "traning iteration: 52 ,loss: 58692.262086, vali_rmse: 1.277498\n",
      "traning iteration: 53 ,loss: 57097.790302, vali_rmse: 1.261030\n",
      "traning iteration: 54 ,loss: 55531.518753, vali_rmse: 1.244655\n",
      "traning iteration: 55 ,loss: 54039.960973, vali_rmse: 1.228861\n",
      "traning iteration: 56 ,loss: 52661.652692, vali_rmse: 1.214078\n",
      "traning iteration: 57 ,loss: 51422.390137, vali_rmse: 1.200615\n",
      "traning iteration: 58 ,loss: 50333.064511, vali_rmse: 1.188633\n",
      "traning iteration: 59 ,loss: 49389.993035, vali_rmse: 1.178132\n",
      "traning iteration: 60 ,loss: 48577.345205, vali_rmse: 1.168974\n",
      "traning iteration: 61 ,loss: 47871.019535, vali_rmse: 1.160921\n",
      "traning iteration: 62 ,loss: 47243.182831, vali_rmse: 1.153685\n",
      "traning iteration: 63 ,loss: 46666.670075, vali_rmse: 1.146978\n",
      "traning iteration: 64 ,loss: 46118.562502, vali_rmse: 1.140552\n",
      "traning iteration: 65 ,loss: 45582.488961, vali_rmse: 1.134232\n",
      "traning iteration: 66 ,loss: 45049.480372, vali_rmse: 1.127919\n",
      "traning iteration: 67 ,loss: 44517.486972, vali_rmse: 1.121597\n",
      "traning iteration: 68 ,loss: 43989.886348, vali_rmse: 1.115307\n",
      "traning iteration: 69 ,loss: 43473.430678, vali_rmse: 1.109131\n",
      "traning iteration: 70 ,loss: 42976.095060, vali_rmse: 1.103166\n",
      "traning iteration: 71 ,loss: 42505.212156, vali_rmse: 1.097502\n",
      "traning iteration: 72 ,loss: 42066.145229, vali_rmse: 1.092203\n",
      "traning iteration: 73 ,loss: 41661.601110, vali_rmse: 1.087307\n",
      "traning iteration: 74 ,loss: 41291.550271, vali_rmse: 1.082815\n",
      "traning iteration: 75 ,loss: 40953.624971, vali_rmse: 1.078700\n",
      "traning iteration: 76 ,loss: 40643.816832, vali_rmse: 1.074916\n",
      "traning iteration: 77 ,loss: 40357.288889, vali_rmse: 1.071406\n",
      "traning iteration: 78 ,loss: 40089.143772, vali_rmse: 1.068111\n",
      "traning iteration: 79 ,loss: 39835.035643, vali_rmse: 1.064978\n",
      "traning iteration: 80 ,loss: 39591.565791, vali_rmse: 1.061967\n",
      "traning iteration: 81 ,loss: 39356.450325, vali_rmse: 1.059050\n",
      "traning iteration: 82 ,loss: 39128.486484, vali_rmse: 1.056211\n",
      "traning iteration: 83 ,loss: 38907.368755, vali_rmse: 1.053447\n",
      "traning iteration: 84 ,loss: 38693.416943, vali_rmse: 1.050760\n",
      "traning iteration: 85 ,loss: 38487.277526, vali_rmse: 1.048157\n",
      "traning iteration: 86 ,loss: 38289.649795, vali_rmse: 1.045649\n",
      "traning iteration: 87 ,loss: 38101.073114, vali_rmse: 1.043240\n",
      "traning iteration: 88 ,loss: 37921.794298, vali_rmse: 1.040934\n",
      "traning iteration: 89 ,loss: 37751.717633, vali_rmse: 1.038730\n",
      "traning iteration: 90 ,loss: 37590.426691, vali_rmse: 1.036625\n",
      "traning iteration: 91 ,loss: 37437.258136, vali_rmse: 1.034610\n",
      "traning iteration: 92 ,loss: 37291.403701, vali_rmse: 1.032677\n",
      "traning iteration: 93 ,loss: 37152.017131, vali_rmse: 1.030819\n",
      "traning iteration: 94 ,loss: 37018.307091, vali_rmse: 1.029026\n",
      "traning iteration: 95 ,loss: 36889.603478, vali_rmse: 1.027293\n",
      "traning iteration: 96 ,loss: 36765.391800, vali_rmse: 1.025615\n",
      "traning iteration: 97 ,loss: 36645.316829, vali_rmse: 1.023990\n",
      "traning iteration: 98 ,loss: 36529.161690, vali_rmse: 1.022416\n",
      "traning iteration: 99 ,loss: 36416.811404, vali_rmse: 1.020894\n"
     ]
    }
   ],
   "source": [
    "lambda_alpha = 0.1\n",
    "lambda_beta = 0.1\n",
    "latent_size = 30\n",
    "lr = 3e-5\n",
    "iters = 100\n",
    "model = PMF(R=train_R, lambda_alpha=lambda_alpha, lambda_beta=lambda_beta, latent_size=latent_size, momuntum=0.9, lr=lr, iters=iters, seed=1)\n",
    "print('parameters are: reg_u={:f}, reg_v={:f}, latent_size={:d}, lr={:f}, iters={:d}'.format(lambda_alpha, lambda_beta, latent_size,lr, iters))\n",
    "U, V, train_loss_list, vali_rmse_list = model.train(train_data=train_R, vali_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained PMF\n",
    "class PMF_2():\n",
    "  def __init__(self, train_R = None, constrained = False ):\n",
    "    self.num_feat = 30\n",
    "    self.momentum = 0.9\n",
    "    self.learning_rate = 0.0005\n",
    "    self.itr = 80\n",
    "    self.lambda_V = 0.001\n",
    "    self.lambda_U = 0.001\n",
    "    self.lambda_W = 0.001\n",
    "    self.train_R = train_R\n",
    "    self.constrained = constrained\n",
    "\n",
    "    self.train_I = copy.deepcopy(self.train_R)\n",
    "    self.train_I[self.train_I>0] = 1\n",
    "\n",
    "    rand_num1 = np.random.RandomState(100)\n",
    "    rand_num2 = np.random.RandomState(101)\n",
    "    \n",
    "    self.U = rand_num1.randn(self.num_feat,train_R.shape[0])\n",
    "    self.V = rand_num1.randn(self.num_feat,train_R.shape[1])\n",
    "    self.W = rand_num2.randn(self.num_feat,train_R.shape[1])\n",
    "\n",
    "    self.Y = None\n",
    "    self.sigma_I = np.dot(self.train_I,np.ones(self.train_R.shape[1]))\n",
    "    self.sigma_I[self.sigma_I == 0] = np.inf\n",
    "\n",
    "\n",
    "  def loss_function(self):\n",
    "    if not self.constrained:\n",
    "      return 0.5*np.sum((self.train_I*(self.train_R-np.dot(self.U.T,self.V))**2)) + 0.5*(self.lambda_U)*np.sum(np.square(self.U)) + 0.5*(self.lambda_V)*np.sum(np.square(self.V))\n",
    "    else:\n",
    "      return 0.5 * np.sum(self.train_I*(self.train_R- np.dot(((self.U+(np.dot(self.W,self.train_I.T)/self.sigma_I)).T),self.V))**2) + (self.lambda_U/2)*np.sum((self.U)**2) + (self.lambda_V/2)*np.sum((self.V**2)) + (self.lambda_W/2)*np.sum((self.W)**2)\n",
    "\n",
    "  \n",
    "  def predict(self,test_data):\n",
    "    pred_value = []\n",
    "    if (self.constrained):\n",
    "      sigma_I = np.dot(self.train_I,np.ones(self.train_R.shape[1]))\n",
    "      sigma_I[self.sigma_I == 0] = np.inf\n",
    "      self.Y = self.U + (np.dot(self.W, self.train_I.T)/sigma_I)\n",
    "      for locate in test_data:\n",
    "        pred_value.append(np.dot(self.Y[:,locate[0]],self.V[:,locate[1]]))\n",
    "\n",
    "    else:\n",
    "      for locate in test_data:\n",
    "        pred_value.append(np.dot(self.U[:,locate[0]],self.V[:,locate[1]]))\n",
    "\n",
    "    return np.array(pred_value)\n",
    "\n",
    "  def fit(self, test_data, df_matrix):\n",
    "    train_mse = []\n",
    "    test_mse = []\n",
    "\n",
    "    mmt_U = np.zeros(self.U.shape)\n",
    "    mmt_V = np.zeros(self.V.shape)\n",
    "\n",
    "    if not self.constrained:\n",
    "      for i in range (self.itr):\n",
    "        dv_u = - (np.dot((self.train_I*(self.train_R - np.dot(self.U.T,self.V))),self.V.T)).T + self.lambda_U*self.U\n",
    "        dv_v = - (np.dot(self.U, (self.train_I*(self.train_R - np.dot(self.U.T, self.V))))) + self.lambda_V*self.V\n",
    "\n",
    "        mmt_U = (self.momentum * mmt_U) + self.learning_rate * dv_u\n",
    "        mmt_V = (self.momentum * mmt_V) + self.learning_rate * dv_v\n",
    "        self.U = self.U - mmt_U * self.momentum\n",
    "        self.V = self.V - mmt_V * self.momentum\n",
    "        train_mse_loss = self.loss_function()\n",
    "        train_mse.append(train_mse_loss)\n",
    "\n",
    "        test_predicts = self.predict(test_data)\n",
    "        test_data_values = df_matrix[test_data[:,0],test_data[:,1]]\n",
    "        test_mse_value =np.sum(np.square(test_data_values-test_predicts))/len(test_data_values)\n",
    "        test_mse.append(test_mse_value)\n",
    "\n",
    "        print('traning iteration:{: d} ,loss:{: f}, test_rmse:{: f}'.format(i, train_mse_loss, test_mse_value))\n",
    "\n",
    "\n",
    "    else:\n",
    "      mmt_W = np.zeros(self.W.shape)\n",
    "      for i in range(self.itr):\n",
    "\n",
    "        dv_u = - (np.dot(self.train_I*(self.train_R-np.dot(self.U.T + (np.dot(self.W,self.train_I.T)/self.sigma_I).T,self.V)),self.V.T)).T + self.lambda_U*self.U\n",
    "        dv_v = - np.dot(self.U ,(self.train_I*(self.train_R-np.dot(self.U.T + (np.dot(self.W, self.train_I.T)/self.sigma_I).T, self.V)))) + self.lambda_V*self.V\n",
    "        #dv_v = - (np.dot(self.train_I*(self.train_R-np.dot(self.U.T + (np.dot((self.W,self.train_I.T)/self.sigma_I).T),self.V),self.U.T+(np.dot(self.W,self.train_I.T)/self.sigma_I).T))).T + self.lambda_V*self.V\n",
    "        dv_w = - (np.dot((self.train_I*(self.train_R-np.dot(self.U.T + (np.dot(self.W,self.train_I.T)/self.sigma_I).T,self.V))).T, np.dot(self.train_I/self.sigma_I.reshape(-1,1),self.V.T))).T + self.lambda_W*self.W\n",
    "\n",
    "        mmt_U = (self.momentum * mmt_U) + self.learning_rate * dv_u\n",
    "        mmt_V = (self.momentum * mmt_V) + self.learning_rate * dv_v\n",
    "        mmt_W = (self.momentum * mmt_W) + self.learning_rate * dv_w\n",
    "        self.U = self.U - self.momentum * mmt_U\n",
    "        self.V = self.V - self.momentum * mmt_V\n",
    "        self.W = self.V - self.momentum * mmt_W\n",
    "        train_mse_loss = self.loss_function()\n",
    "        train_mse.append(train_mse_loss)\n",
    "\n",
    "        test_predicts = self.predict(test_data)\n",
    "        test_data_values = df_matrix[test_data[:,0],test_data[:,1]]\n",
    "        test_mse_value =np.sum(np.square(test_data_values-test_predicts))/len(test_data_values)\n",
    "        test_mse.append(test_mse_value)\n",
    "\n",
    "        print('traning iteration:{: d} ,loss:{: f}, test_rmse:{: f}'.format(i, train_mse_loss, test_mse_value))\n",
    "\n",
    "\n",
    "\n",
    "      pass\n",
    "\n",
    "    return self.U, self.V, train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 3. 4. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [5. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess ratings into R\n",
    "import math\n",
    "df = pd.DataFrame(ratings, columns=['user', 'item', 'rating'])\n",
    "df = df.pivot_table(index='user', columns='item', values ='rating')\n",
    "df_matrix = df.to_numpy()\n",
    "locate = []\n",
    "for i in range (len(df_matrix)):\n",
    "  for j in range (len(df_matrix[0])):\n",
    "    if not(math.isnan(df_matrix[i][j])):\n",
    "      locate.append((i,j))\n",
    "data = np.array(locate)\n",
    "data = data.astype(int)\n",
    "\n",
    "train_data, test_data = train_test_split(data,test_size = 0.25, random_state = 42)\n",
    "train_df = np.zeros(df_matrix.shape)\n",
    "test_df = np.zeros(df_matrix.shape)\n",
    "\n",
    "for tup in train_data:\n",
    "  train_df[tup[0]][tup[1]] = df_matrix[tup[0]][tup[1]]\n",
    "for tup in test_data:\n",
    "  test_df[tup[0]][tup[1]] = df_matrix[tup[0]][tup[1]]\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[692, 381],\n",
       "       [746, 110],\n",
       "       [200, 211],\n",
       "       ...,\n",
       "       [756,  23],\n",
       "       [591, 930],\n",
       "       [108,  97]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 0 ,loss: 1255482.448738, test_rmse: 38.111642\n",
      "traning iteration: 1 ,loss: 887682.362886, test_rmse: 29.780032\n",
      "traning iteration: 2 ,loss: 631667.140073, test_rmse: 23.373577\n",
      "traning iteration: 3 ,loss: 492630.371940, test_rmse: 19.301712\n",
      "traning iteration: 4 ,loss: 416179.282208, test_rmse: 16.599914\n",
      "traning iteration: 5 ,loss: 356716.518384, test_rmse: 14.325277\n",
      "traning iteration: 6 ,loss: 300166.238347, test_rmse: 12.169943\n",
      "traning iteration: 7 ,loss: 267325.679194, test_rmse: 10.684963\n",
      "traning iteration: 8 ,loss: 268469.495499, test_rmse: 10.240280\n",
      "traning iteration: 9 ,loss: 251839.876366, test_rmse: 9.605368\n",
      "traning iteration: 10 ,loss: 208495.808735, test_rmse: 8.407351\n",
      "traning iteration: 11 ,loss: 184023.017493, test_rmse: 7.636058\n",
      "traning iteration: 12 ,loss: 178267.772264, test_rmse: 7.278347\n",
      "traning iteration: 13 ,loss: 170755.637122, test_rmse: 6.866471\n",
      "traning iteration: 14 ,loss: 153758.234012, test_rmse: 6.238725\n",
      "traning iteration: 15 ,loss: 131999.992233, test_rmse: 5.522317\n",
      "traning iteration: 16 ,loss: 113271.585487, test_rmse: 4.909278\n",
      "traning iteration: 17 ,loss: 100143.545932, test_rmse: 4.456962\n",
      "traning iteration: 18 ,loss: 89891.743364, test_rmse: 4.087761\n",
      "traning iteration: 19 ,loss: 80923.706966, test_rmse: 3.756257\n",
      "traning iteration: 20 ,loss: 74190.108204, test_rmse: 3.488856\n",
      "traning iteration: 21 ,loss: 69698.961184, test_rmse: 3.290274\n",
      "traning iteration: 22 ,loss: 65905.262646, test_rmse: 3.122581\n",
      "traning iteration: 23 ,loss: 61616.413240, test_rmse: 2.953127\n",
      "traning iteration: 24 ,loss: 56961.083548, test_rmse: 2.781439\n",
      "traning iteration: 25 ,loss: 52757.619928, test_rmse: 2.626125\n",
      "traning iteration: 26 ,loss: 49420.308121, test_rmse: 2.498656\n",
      "traning iteration: 27 ,loss: 46641.089921, test_rmse: 2.393677\n",
      "traning iteration: 28 ,loss: 43995.776942, test_rmse: 2.301116\n",
      "traning iteration: 29 ,loss: 41461.131374, test_rmse: 2.218148\n",
      "traning iteration: 30 ,loss: 39219.701880, test_rmse: 2.146130\n",
      "traning iteration: 31 ,loss: 37292.712750, test_rmse: 2.083495\n",
      "traning iteration: 32 ,loss: 35546.959919, test_rmse: 2.026416\n",
      "traning iteration: 33 ,loss: 33895.485083, test_rmse: 1.973064\n",
      "traning iteration: 34 ,loss: 32353.097123, test_rmse: 1.924123\n",
      "traning iteration: 35 ,loss: 30946.837879, test_rmse: 1.880202\n",
      "traning iteration: 36 ,loss: 29664.189218, test_rmse: 1.840695\n",
      "traning iteration: 37 ,loss: 28490.245970, test_rmse: 1.805026\n",
      "traning iteration: 38 ,loss: 27434.588282, test_rmse: 1.773490\n",
      "traning iteration: 39 ,loss: 26499.118572, test_rmse: 1.746388\n",
      "traning iteration: 40 ,loss: 25652.292815, test_rmse: 1.723210\n",
      "traning iteration: 41 ,loss: 24853.473955, test_rmse: 1.703073\n",
      "traning iteration: 42 ,loss: 24084.183531, test_rmse: 1.685450\n",
      "traning iteration: 43 ,loss: 23346.496323, test_rmse: 1.670220\n",
      "traning iteration: 44 ,loss: 22649.752037, test_rmse: 1.657456\n",
      "traning iteration: 45 ,loss: 22010.656339, test_rmse: 1.647446\n",
      "traning iteration: 46 ,loss: 21449.827227, test_rmse: 1.640543\n",
      "traning iteration: 47 ,loss: 20969.036363, test_rmse: 1.636566\n",
      "traning iteration: 48 ,loss: 20536.260339, test_rmse: 1.634466\n",
      "traning iteration: 49 ,loss: 20107.708610, test_rmse: 1.632928\n",
      "traning iteration: 50 ,loss: 19665.864839, test_rmse: 1.631325\n",
      "traning iteration: 51 ,loss: 19228.753461, test_rmse: 1.629940\n",
      "traning iteration: 52 ,loss: 18823.342911, test_rmse: 1.629311\n",
      "traning iteration: 53 ,loss: 18458.197181, test_rmse: 1.629571\n",
      "traning iteration: 54 ,loss: 18122.888696, test_rmse: 1.630451\n",
      "traning iteration: 55 ,loss: 17804.743783, test_rmse: 1.631681\n",
      "traning iteration: 56 ,loss: 17499.008084, test_rmse: 1.633209\n",
      "traning iteration: 57 ,loss: 17206.140881, test_rmse: 1.635099\n",
      "traning iteration: 58 ,loss: 16926.786458, test_rmse: 1.637390\n",
      "traning iteration: 59 ,loss: 16661.101797, test_rmse: 1.640077\n",
      "traning iteration: 60 ,loss: 16409.262428, test_rmse: 1.643139\n",
      "traning iteration: 61 ,loss: 16170.540898, test_rmse: 1.646533\n",
      "traning iteration: 62 ,loss: 15943.366671, test_rmse: 1.650212\n",
      "traning iteration: 63 ,loss: 15727.223367, test_rmse: 1.654168\n",
      "traning iteration: 64 ,loss: 15523.001765, test_rmse: 1.658437\n",
      "traning iteration: 65 ,loss: 15330.419128, test_rmse: 1.663020\n",
      "traning iteration: 66 ,loss: 15146.333592, test_rmse: 1.667824\n",
      "traning iteration: 67 ,loss: 14967.064558, test_rmse: 1.672716\n",
      "traning iteration: 68 ,loss: 14791.917322, test_rmse: 1.677616\n",
      "traning iteration: 69 ,loss: 14623.211865, test_rmse: 1.682526\n",
      "traning iteration: 70 ,loss: 14462.959977, test_rmse: 1.687471\n",
      "traning iteration: 71 ,loss: 14310.573772, test_rmse: 1.692450\n",
      "traning iteration: 72 ,loss: 14163.981931, test_rmse: 1.697458\n",
      "traning iteration: 73 ,loss: 14021.965394, test_rmse: 1.702518\n",
      "traning iteration: 74 ,loss: 13884.721035, test_rmse: 1.707680\n",
      "traning iteration: 75 ,loss: 13752.656201, test_rmse: 1.712979\n",
      "traning iteration: 76 ,loss: 13625.455802, test_rmse: 1.718407\n",
      "traning iteration: 77 ,loss: 13502.381146, test_rmse: 1.723919\n",
      "traning iteration: 78 ,loss: 13382.941213, test_rmse: 1.729460\n",
      "traning iteration: 79 ,loss: 13267.001095, test_rmse: 1.734973\n"
     ]
    }
   ],
   "source": [
    "model1 = PMF_2(train_R = train_df, constrained= True)\n",
    "U1, V1, tr_loss_list1, test_rmse_list1 = model1.fit(test_data, df_matrix)\n",
    "preds = model1.predict(test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
