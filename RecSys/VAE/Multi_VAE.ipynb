{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbw82\\AppData\\Local\\Temp\\ipykernel_15572\\1961604861.py:18: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_movies = pd.read_csv(file_path+'./ml-1m/movies.dat', sep = \"::\", names = ['movieid','title','genres'], encoding_errors='ignore')\n",
      "C:\\Users\\kbw82\\AppData\\Local\\Temp\\ipykernel_15572\\1961604861.py:19: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_ratings = pd.read_csv(file_path+'./ml-1m/ratings.dat', sep = \"::\", names = ['userid','movieid','ratings','timestamp'], encoding_errors='ignore')\n",
      "C:\\Users\\kbw82\\AppData\\Local\\Temp\\ipykernel_15572\\1961604861.py:20: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_users = pd.read_csv(file_path+'./ml-1m/users.dat', sep = \"::\", names = ['userid','gender','age','occupation','zipcode'], encoding_errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "file_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ========================================\n",
    "# Load MovieLens 1M datasets\n",
    "# ========================================\n",
    "data_movies = pd.read_csv(file_path+'./ml-1m/movies.dat', sep = \"::\", names = ['movieid','title','genres'], encoding_errors='ignore')\n",
    "data_ratings = pd.read_csv(file_path+'./ml-1m/ratings.dat', sep = \"::\", names = ['userid','movieid','ratings','timestamp'], encoding_errors='ignore')\n",
    "data_users = pd.read_csv(file_path+'./ml-1m/users.dat', sep = \"::\", names = ['userid','gender','age','occupation','zipcode'], encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kbw82\\Desktop\\2024-DSAIL-Lab-Intern\\labenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from box import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    'p_dims': [200, 600],\n",
    "    'dropout_rate' : 0.5,\n",
    "    'weight_decay' : 0.01,\n",
    "    'valid_samples' : 10,\n",
    "    'seed' : 22,\n",
    "    'anneal_cap' : 0.2,\n",
    "    'total_anneal_steps' : 200000,\n",
    "\n",
    "    'lr' : 0.005,\n",
    "    'batch_size' : 500,\n",
    "    'num_epochs' : 50,\n",
    "    'num_workers' : 0,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeMatrixDataSet():\n",
    "    def __init__(self, ratings):\n",
    "        \n",
    "        self.df = ratings\n",
    "        \n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieid')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userid')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['movieid'].apply(lambda x : self.item_encoder[x])\n",
    "        self.df['user_idx'] = self.df['userid'].apply(lambda x : self.user_encoder[x])\n",
    "\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['timestamp']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(42)\n",
    "\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = 10, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid \n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def make_matrix(self, user_list, train = True):\n",
    "        \"\"\"\n",
    "        user_item_matrix based on input user_list\n",
    "        \"\"\"\n",
    "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
    "        for idx, user in enumerate(user_list):\n",
    "            if train:\n",
    "                mat[idx, self.user_train[user.item()]] = 1\n",
    "            else:\n",
    "                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n",
    "\n",
    "        return mat\n",
    "    \n",
    "class AEDataSet(Dataset):\n",
    "    def __init__(self, num_user):\n",
    "        self.num_user = num_user\n",
    "        self.users = [i for i in range(num_user)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        user = self.users[idx]\n",
    "        return torch.LongTensor([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, p_dims, dropout_rate = 0.5):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims = p_dims[::-1]\n",
    "\n",
    "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
    "\n",
    "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
    "\n",
    "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
    "\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input, loss = False):\n",
    "        mu, logvar = self.encode(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        h = self.decode(z)\n",
    "        if loss:\n",
    "            return h, mu, logvar\n",
    "        else:\n",
    "            return h\n",
    "    \n",
    "    def encode(self, input):\n",
    "        h = F.normalize(input)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        for i, layer in enumerate(self.q_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.q_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "            else:\n",
    "                mu = h[:, :self.q_dims[-1]]\n",
    "                logvar = h[:, self.q_dims[-1]:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = z\n",
    "        for i, layer in enumerate(self.p_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.p_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.q_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "        \n",
    "        for layer in self.p_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunc(nn.Module):\n",
    "\n",
    "    def __init__(self, loss_type = 'Multinomial', model_type = None):\n",
    "        super(LossFunc, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def forward(self, recon_x = None, x = None, mu = None, logvar = None, anneal = None):\n",
    "        if self.loss_type == 'Gaussian':\n",
    "            loss = self.Gaussian(recon_x, x)\n",
    "        elif self.loss_type == 'Logistic':\n",
    "            loss = self.Logistic(recon_x, x)\n",
    "        elif self.loss_type == 'Multinomial':\n",
    "            loss = self.Multinomial(recon_x, x)\n",
    "        \n",
    "        if self.model_type == 'VAE':\n",
    "            KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "            loss = loss + anneal * KLD\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def Gaussian(self, recon_x, x):\n",
    "        gaussian = F.mse_loss(recon_x, x)\n",
    "        return gaussian\n",
    "\n",
    "    def Logistic(self, recon_x, x):\n",
    "        logistic = F.binary_cross_entropy(recon_x.sigmoid(), x, reduction='none').sum(1).mean()\n",
    "        return logistic\n",
    "\n",
    "    def Multinomial(self, recon_x, x):\n",
    "        # putting log in pmf and omit the factorial part, only calculates the sum of x * p_i\n",
    "        multinomial = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "        return multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(1, len(pred_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, data_loader, make_matrix_data_set, config):\n",
    "    global update_count\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for users in data_loader:\n",
    "        mat = make_matrix_data_set.make_matrix(users)\n",
    "        mat = mat.to(device)\n",
    "\n",
    "        if criterion.model_type == 'VAE':\n",
    "            anneal = min(config.anneal_cap, 1. * update_count / config.total_anneal_steps)\n",
    "            update_count += 1\n",
    "            recon_mat, mu, logvar = model(mat, loss = True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(recon_x = recon_mat, x = mat, mu = mu, logvar = logvar, anneal = anneal)\n",
    "\n",
    "        else:\n",
    "            recon_mat = model(mat)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(recon_x = recon_mat, x = mat)\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader, user_train, user_valid, make_matrix_data_set):\n",
    "    model.eval()\n",
    "\n",
    "    NDCG = 0.0 # NDCG@10\n",
    "    HIT = 0.0 # HIT@10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users in data_loader:\n",
    "            mat = make_matrix_data_set.make_matrix(users)\n",
    "            mat = mat.to(device)\n",
    "\n",
    "            recon_mat = model(mat)\n",
    "            recon_mat[mat == 1] = -np.inf\n",
    "            rec_list = recon_mat.argsort(dim = 1)\n",
    "\n",
    "            for user, rec in zip(users, rec_list):\n",
    "                uv = user_valid[user.item()]\n",
    "                up = rec[-10:].cpu().numpy().tolist()\n",
    "                NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
    "                HIT += get_hit(pred_list = up, true_list = uv)\n",
    "\n",
    "    NDCG /= len(data_loader.dataset)\n",
    "    HIT /= len(data_loader.dataset)\n",
    "\n",
    "    return NDCG, HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {}\n",
    "ndcg_dict = {}\n",
    "hit_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix_data_set = MakeMatrixDataSet(data_ratings)\n",
    "user_train, user_valid = make_matrix_data_set.get_train_valid_data()\n",
    "ae_dataset = AEDataSet(\n",
    "    num_user = make_matrix_data_set.num_user,\n",
    "    )\n",
    "\n",
    "# 500 users batch\n",
    "data_loader = DataLoader(\n",
    "    ae_dataset,\n",
    "    batch_size = 500, \n",
    "    shuffle = True, \n",
    "    pin_memory = True,\n",
    "    num_workers = 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE with Logistic Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    p_dims = config.p_dims + [make_matrix_data_set.num_item], \n",
    "    dropout_rate = config.dropout_rate).to(device)\n",
    "\n",
    "criterion = LossFunc(loss_type = 'Logistic', model_type = 'VAE')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 1046.69032| NDCG@10: 0.05430| HIT@10: 0.04553\n",
      "Epoch:   2| Train loss: 562.90932| NDCG@10: 0.07126| HIT@10: 0.06000\n",
      "Epoch:   3| Train loss: 544.27723| NDCG@10: 0.07817| HIT@10: 0.06666\n",
      "Epoch:   4| Train loss: 517.95200| NDCG@10: 0.08403| HIT@10: 0.06970\n",
      "Epoch:   5| Train loss: 496.85051| NDCG@10: 0.08579| HIT@10: 0.07219\n",
      "Epoch:   6| Train loss: 479.96840| NDCG@10: 0.07854| HIT@10: 0.06589\n",
      "Epoch:   7| Train loss: 470.85446| NDCG@10: 0.08613| HIT@10: 0.07139\n",
      "Epoch:   8| Train loss: 460.70295| NDCG@10: 0.08206| HIT@10: 0.06899\n",
      "Epoch:   9| Train loss: 451.70306| NDCG@10: 0.07891| HIT@10: 0.06634\n",
      "Epoch:  10| Train loss: 446.84652| NDCG@10: 0.08449| HIT@10: 0.07190\n",
      "Epoch:  11| Train loss: 443.02700| NDCG@10: 0.08173| HIT@10: 0.06667\n",
      "Epoch:  12| Train loss: 445.67227| NDCG@10: 0.08458| HIT@10: 0.06993\n",
      "Epoch:  13| Train loss: 437.99890| NDCG@10: 0.08361| HIT@10: 0.07017\n",
      "Epoch:  14| Train loss: 446.54483| NDCG@10: 0.08357| HIT@10: 0.07086\n",
      "Epoch:  15| Train loss: 443.04455| NDCG@10: 0.08382| HIT@10: 0.06975\n",
      "Epoch:  16| Train loss: 431.57288| NDCG@10: 0.08365| HIT@10: 0.07109\n",
      "Epoch:  17| Train loss: 438.75646| NDCG@10: 0.07773| HIT@10: 0.06710\n",
      "Epoch:  18| Train loss: 453.60964| NDCG@10: 0.07976| HIT@10: 0.06661\n",
      "Epoch:  19| Train loss: 458.03356| NDCG@10: 0.08650| HIT@10: 0.06891\n",
      "Epoch:  20| Train loss: 464.34408| NDCG@10: 0.07340| HIT@10: 0.06094\n",
      "Epoch:  21| Train loss: 455.36272| NDCG@10: 0.07616| HIT@10: 0.06328\n",
      "Epoch:  22| Train loss: 446.14506| NDCG@10: 0.07744| HIT@10: 0.06487\n",
      "Epoch:  23| Train loss: 448.33993| NDCG@10: 0.07815| HIT@10: 0.06623\n",
      "Epoch:  24| Train loss: 443.18412| NDCG@10: 0.08221| HIT@10: 0.07000\n",
      "Epoch:  25| Train loss: 434.62839| NDCG@10: 0.07682| HIT@10: 0.06694\n",
      "Epoch:  26| Train loss: 440.18734| NDCG@10: 0.07246| HIT@10: 0.06018\n",
      "Epoch:  27| Train loss: 434.46970| NDCG@10: 0.08200| HIT@10: 0.06912\n",
      "Epoch:  28| Train loss: 439.03620| NDCG@10: 0.07875| HIT@10: 0.06583\n",
      "Epoch:  29| Train loss: 435.80972| NDCG@10: 0.08610| HIT@10: 0.07141\n",
      "Epoch:  30| Train loss: 431.74866| NDCG@10: 0.07721| HIT@10: 0.06735\n",
      "Epoch:  31| Train loss: 434.97651| NDCG@10: 0.09453| HIT@10: 0.07975\n",
      "Epoch:  32| Train loss: 425.12596| NDCG@10: 0.10632| HIT@10: 0.09088\n",
      "Epoch:  33| Train loss: 410.81512| NDCG@10: 0.11498| HIT@10: 0.09868\n",
      "Epoch:  34| Train loss: 398.73666| NDCG@10: 0.11813| HIT@10: 0.10270\n",
      "Epoch:  35| Train loss: 397.19411| NDCG@10: 0.12465| HIT@10: 0.10773\n",
      "Epoch:  36| Train loss: 388.77276| NDCG@10: 0.12198| HIT@10: 0.10498\n",
      "Epoch:  37| Train loss: 394.49759| NDCG@10: 0.12016| HIT@10: 0.10377\n",
      "Epoch:  38| Train loss: 392.20012| NDCG@10: 0.12478| HIT@10: 0.10740\n",
      "Epoch:  39| Train loss: 387.54531| NDCG@10: 0.12633| HIT@10: 0.10929\n",
      "Epoch:  40| Train loss: 384.40083| NDCG@10: 0.12590| HIT@10: 0.10871\n",
      "Epoch:  41| Train loss: 380.81839| NDCG@10: 0.13490| HIT@10: 0.11623\n",
      "Epoch:  42| Train loss: 372.79295| NDCG@10: 0.13488| HIT@10: 0.11738\n",
      "Epoch:  43| Train loss: 377.18100| NDCG@10: 0.14165| HIT@10: 0.12065\n",
      "Epoch:  44| Train loss: 380.50297| NDCG@10: 0.14712| HIT@10: 0.12667\n",
      "Epoch:  45| Train loss: 366.65734| NDCG@10: 0.14826| HIT@10: 0.12767\n",
      "Epoch:  46| Train loss: 360.91098| NDCG@10: 0.15125| HIT@10: 0.13108\n",
      "Epoch:  47| Train loss: 370.88208| NDCG@10: 0.15726| HIT@10: 0.13599\n",
      "Epoch:  48| Train loss: 370.01455| NDCG@10: 0.15022| HIT@10: 0.12939\n",
      "Epoch:  49| Train loss: 365.21156| NDCG@10: 0.15670| HIT@10: 0.13422\n",
      "Epoch:  50| Train loss: 356.42479| NDCG@10: 0.15889| HIT@10: 0.13487\n"
     ]
    }
   ],
   "source": [
    "best_hit = 0\n",
    "update_count = 1\n",
    "loss_list = []\n",
    "ndcg_list = []\n",
    "hit_list = []\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    \n",
    "    train_loss = train(\n",
    "        model = model, \n",
    "        criterion = criterion, \n",
    "        optimizer = optimizer, \n",
    "        data_loader = data_loader,\n",
    "        make_matrix_data_set = make_matrix_data_set,\n",
    "        config = config,\n",
    "        )\n",
    "    \n",
    "    ndcg, hit = evaluate(\n",
    "        model = model, \n",
    "        data_loader = data_loader,\n",
    "        user_train = user_train,\n",
    "        user_valid = user_valid,\n",
    "        make_matrix_data_set = make_matrix_data_set,\n",
    "        )\n",
    "\n",
    "    loss_list.append(train_loss)\n",
    "    ndcg_list.append(ndcg)\n",
    "    hit_list.append(hit)\n",
    "\n",
    "    print(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    p_dims = config.p_dims + [make_matrix_data_set.num_item], \n",
    "    dropout_rate = config.dropout_rate).to(device)\n",
    "\n",
    "criterion = LossFunc(loss_type = 'Multinomial', model_type = 'VAE')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kbw82\\Desktop\\2024-DSAIL-Lab-Intern\\labenv\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 1180.00846| NDCG@10: 0.08561| HIT@10: 0.07157\n",
      "Epoch:   2| Train loss: 1158.18674| NDCG@10: 0.12077| HIT@10: 0.10230\n",
      "Epoch:   3| Train loss: 1141.33620| NDCG@10: 0.14885| HIT@10: 0.12773\n",
      "Epoch:   4| Train loss: 1108.19178| NDCG@10: 0.16510| HIT@10: 0.14179\n",
      "Epoch:   5| Train loss: 1097.91410| NDCG@10: 0.17916| HIT@10: 0.15232\n",
      "Epoch:   6| Train loss: 1066.42540| NDCG@10: 0.18532| HIT@10: 0.15828\n",
      "Epoch:   7| Train loss: 1093.40770| NDCG@10: 0.18975| HIT@10: 0.16131\n",
      "Epoch:   8| Train loss: 1074.27530| NDCG@10: 0.19722| HIT@10: 0.16849\n",
      "Epoch:   9| Train loss: 1090.46537| NDCG@10: 0.19813| HIT@10: 0.16940\n",
      "Epoch:  10| Train loss: 1070.13530| NDCG@10: 0.20431| HIT@10: 0.17432\n",
      "Epoch:  11| Train loss: 1072.67084| NDCG@10: 0.20402| HIT@10: 0.17364\n",
      "Epoch:  12| Train loss: 1048.56078| NDCG@10: 0.21197| HIT@10: 0.18179\n",
      "Epoch:  13| Train loss: 1072.10419| NDCG@10: 0.21245| HIT@10: 0.17977\n",
      "Epoch:  14| Train loss: 1072.47389| NDCG@10: 0.20758| HIT@10: 0.17599\n",
      "Epoch:  15| Train loss: 1056.60491| NDCG@10: 0.20853| HIT@10: 0.17618\n",
      "Epoch:  16| Train loss: 1064.47174| NDCG@10: 0.20891| HIT@10: 0.17629\n",
      "Epoch:  17| Train loss: 1037.00503| NDCG@10: 0.21335| HIT@10: 0.18154\n",
      "Epoch:  18| Train loss: 1020.15732| NDCG@10: 0.21280| HIT@10: 0.18071\n",
      "Epoch:  19| Train loss: 1025.75527| NDCG@10: 0.21281| HIT@10: 0.18036\n",
      "Epoch:  20| Train loss: 1027.23547| NDCG@10: 0.21318| HIT@10: 0.17911\n",
      "Epoch:  21| Train loss: 1025.77257| NDCG@10: 0.21349| HIT@10: 0.17912\n",
      "Epoch:  22| Train loss: 1021.60940| NDCG@10: 0.20915| HIT@10: 0.17576\n",
      "Epoch:  23| Train loss: 1081.99400| NDCG@10: 0.20715| HIT@10: 0.17377\n",
      "Epoch:  24| Train loss: 1025.19467| NDCG@10: 0.20661| HIT@10: 0.17351\n",
      "Epoch:  25| Train loss: 1060.26153| NDCG@10: 0.20344| HIT@10: 0.17169\n",
      "Epoch:  26| Train loss: 1049.84515| NDCG@10: 0.21031| HIT@10: 0.17536\n",
      "Epoch:  27| Train loss: 1064.21014| NDCG@10: 0.19469| HIT@10: 0.16204\n",
      "Epoch:  28| Train loss: 1045.67961| NDCG@10: 0.20063| HIT@10: 0.16810\n",
      "Epoch:  29| Train loss: 1073.55603| NDCG@10: 0.19971| HIT@10: 0.16752\n",
      "Epoch:  30| Train loss: 1037.35346| NDCG@10: 0.20282| HIT@10: 0.17075\n",
      "Epoch:  31| Train loss: 1020.40086| NDCG@10: 0.20112| HIT@10: 0.16780\n",
      "Epoch:  32| Train loss: 1047.21850| NDCG@10: 0.20378| HIT@10: 0.17180\n",
      "Epoch:  33| Train loss: 1021.81841| NDCG@10: 0.19791| HIT@10: 0.16684\n",
      "Epoch:  34| Train loss: 1051.94010| NDCG@10: 0.20236| HIT@10: 0.17023\n",
      "Epoch:  35| Train loss: 1056.17318| NDCG@10: 0.19938| HIT@10: 0.16634\n",
      "Epoch:  36| Train loss: 1052.53129| NDCG@10: 0.20418| HIT@10: 0.17200\n",
      "Epoch:  37| Train loss: 1048.85783| NDCG@10: 0.19715| HIT@10: 0.16599\n",
      "Epoch:  38| Train loss: 1027.16826| NDCG@10: 0.19582| HIT@10: 0.16548\n",
      "Epoch:  39| Train loss: 1042.69591| NDCG@10: 0.19771| HIT@10: 0.16536\n",
      "Epoch:  40| Train loss: 1054.89708| NDCG@10: 0.19848| HIT@10: 0.16568\n",
      "Epoch:  41| Train loss: 1033.01362| NDCG@10: 0.19457| HIT@10: 0.16247\n",
      "Epoch:  42| Train loss: 1030.96225| NDCG@10: 0.19827| HIT@10: 0.16659\n",
      "Epoch:  43| Train loss: 1019.44924| NDCG@10: 0.19568| HIT@10: 0.16525\n",
      "Epoch:  44| Train loss: 1016.38468| NDCG@10: 0.19814| HIT@10: 0.16733\n",
      "Epoch:  45| Train loss: 1009.86992| NDCG@10: 0.20181| HIT@10: 0.16982\n",
      "Epoch:  46| Train loss: 1023.52402| NDCG@10: 0.19821| HIT@10: 0.16589\n",
      "Epoch:  47| Train loss: 1051.40234| NDCG@10: 0.19760| HIT@10: 0.16666\n",
      "Epoch:  48| Train loss: 1013.41583| NDCG@10: 0.19691| HIT@10: 0.16583\n",
      "Epoch:  49| Train loss: 1032.25747| NDCG@10: 0.19993| HIT@10: 0.16810\n",
      "Epoch:  50| Train loss: 1008.15695| NDCG@10: 0.19202| HIT@10: 0.16081\n"
     ]
    }
   ],
   "source": [
    "best_hit = 0\n",
    "update_count = 1\n",
    "loss_list = []\n",
    "ndcg_list = []\n",
    "hit_list = []\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    \n",
    "    train_loss = train(\n",
    "        model = model, \n",
    "        criterion = criterion, \n",
    "        optimizer = optimizer, \n",
    "        data_loader = data_loader,\n",
    "        make_matrix_data_set = make_matrix_data_set,\n",
    "        config = config,\n",
    "        )\n",
    "    \n",
    "    ndcg, hit = evaluate(\n",
    "        model = model, \n",
    "        data_loader = data_loader,\n",
    "        user_train = user_train,\n",
    "        user_valid = user_valid,\n",
    "        make_matrix_data_set = make_matrix_data_set,\n",
    "        )\n",
    "\n",
    "    loss_list.append(train_loss)\n",
    "    ndcg_list.append(ndcg)\n",
    "    hit_list.append(hit)\n",
    "\n",
    "    print(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
